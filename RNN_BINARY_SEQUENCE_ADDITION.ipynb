{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RNN BINARY SEQUENCE ADDITION",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "hJeJ638MuQ6c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a8ed6a2-be67-4ea1-90c2-6f31e5f33004"
      },
      "source": [
        "import copy, numpy as np\n",
        "np.random.seed(0)\n",
        "\n",
        "# sigmoid activation\n",
        "def sigmoid(x):\n",
        "    output = 1/(1+np.exp(-x))\n",
        "    return output\n",
        "\n",
        "# sigmoid derivative\n",
        "def sigmoid_dr(output):\n",
        "    return output*(1-output)\n",
        "\n",
        "# initilizing dictionary for integer to binary \n",
        "int2binary = {}\n",
        "binary_dim = 8 #no of bits or siquence length\n",
        "\n",
        "largest_number = 2**binary_dim\n",
        "binary = np.unpackbits(\n",
        "    np.array([range(largest_number)],dtype=np.uint8).T,axis=1)\n",
        "for i in range(largest_number):\n",
        "    int2binary[i] = binary[i]\n",
        "\n",
        "# input variables\n",
        "alpha = 0.1\n",
        "input_dim = 2\n",
        "hidden_dim = 16\n",
        "output_dim = 1\n",
        "\n",
        "\n",
        "# initialize neural network weights\n",
        "wax = 2*np.random.random((input_dim,hidden_dim)) - 1\n",
        "way = 2*np.random.random((hidden_dim,output_dim)) - 1\n",
        "waa = 2*np.random.random((hidden_dim,hidden_dim)) - 1\n",
        "\n",
        "wax_update = np.zeros_like(wax)\n",
        "way_update = np.zeros_like(way)\n",
        "waa_update = np.zeros_like(waa)\n",
        "\n",
        "# training logic\n",
        "for j in range(10000):\n",
        "    a_int = np.random.randint(largest_number/2)\n",
        "    a = int2binary[a_int] \n",
        "    b_int = np.random.randint(largest_number/2) \n",
        "    b = int2binary[b_int] \n",
        "    c_int = a_int + b_int\n",
        "    c = int2binary[c_int]\n",
        "    d = np.zeros_like(c)\n",
        "\n",
        "    TotalLoss = 0\n",
        "    \n",
        "    drivatives = list()\n",
        "    activation_values = list()#a's\n",
        "    activation_values.append(np.zeros(hidden_dim))\n",
        "    \n",
        "    # moving along the positions in the binary encoding\n",
        "    for position in range(binary_dim):\n",
        "        \n",
        "        # generate input and output\n",
        "        X = np.array([[a[binary_dim - position - 1],b[binary_dim - position - 1]]])\n",
        "        y = np.array([[c[binary_dim - position - 1]]]).T\n",
        "\n",
        "        # a=g(wax*xi+waa*at-1)\n",
        "        at = sigmoid(np.dot(X,wax) + np.dot(activation_values[-1],waa))\n",
        "\n",
        "        # yt=g(way*at)\n",
        "        ypred = sigmoid(np.dot(at,way))\n",
        "        #calculation loss\n",
        "        loss = y - ypred\n",
        "        #dl/da=loss*(a*(1-a))\n",
        "        drivatives.append((loss)*sigmoid_dr(ypred))\n",
        "        TotalLoss += np.abs(loss[0])\n",
        "    \n",
        "        # decode estimate so we can print it out\n",
        "        d[binary_dim - position - 1] = np.round(ypred[0][0])\n",
        "        \n",
        "        # store hidden layer so we can use it in the next timestep\n",
        "        activation_values.append(copy.deepcopy(at))\n",
        "    \n",
        "    final_Activation_drs = np.zeros(hidden_dim)\n",
        "    \n",
        "    for position in range(binary_dim):\n",
        "        \n",
        "        X = np.array([[a[position],b[position]]])\n",
        "        at = activation_values[-position-1]\n",
        "        prev_at = activation_values[-position-2]\n",
        "        \n",
        "        # error at output layer\n",
        "        drivative2 = drivatives[-position-1]\n",
        "        # error at hidden layer\n",
        "        drivative1 = (final_Activation_drs.dot(waa.T) + drivative2.dot(way.T)) * sigmoid_dr(at)\n",
        "\n",
        "        # let's update all our weights so we can try again\n",
        "        way_update += np.atleast_2d(at).T.dot(drivative2)\n",
        "        waa_update += np.atleast_2d(prev_at).T.dot(drivative1)\n",
        "        wax_update += X.T.dot(drivative1)\n",
        "        \n",
        "        final_Activation_drs = drivative1\n",
        "    \n",
        "\n",
        "    wax += wax_update * alpha\n",
        "    way += way_update * alpha\n",
        "    waa += waa_update * alpha    \n",
        "\n",
        "    wax_update *= 0\n",
        "    way_update *= 0\n",
        "    waa_update *= 0\n",
        "    \n",
        "    # print out progress\n",
        "    if(j % 1000 == 0):\n",
        "        print(f\"Error:{ str(TotalLoss)}\")\n",
        "        print(f\"Pred:  {str(d)}\")\n",
        "        print(f\"True:  {str(c)}\")\n",
        "        out = 0\n",
        "        for index,x in enumerate(reversed(d)):\n",
        "            out += x*pow(2,index)\n",
        "        print(f\"{str(a_int)}  +  { str(b_int)}  = str(out)\")\n",
        "        print(\"------------\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Error:[3.45638663]\n",
            "Pred:  [0 0 0 0 0 0 0 1]\n",
            "True:  [0 1 0 0 0 1 0 1]\n",
            "9  +  60  = str(out)\n",
            "------------\n",
            "Error:[3.63389116]\n",
            "Pred:  [1 1 1 1 1 1 1 1]\n",
            "True:  [0 0 1 1 1 1 1 1]\n",
            "28  +  35  = str(out)\n",
            "------------\n",
            "Error:[3.91366595]\n",
            "Pred:  [0 1 0 0 1 0 0 0]\n",
            "True:  [1 0 1 0 0 0 0 0]\n",
            "116  +  44  = str(out)\n",
            "------------\n",
            "Error:[3.72191702]\n",
            "Pred:  [1 1 0 1 1 1 1 1]\n",
            "True:  [0 1 0 0 1 1 0 1]\n",
            "4  +  73  = str(out)\n",
            "------------\n",
            "Error:[3.5852713]\n",
            "Pred:  [0 0 0 0 1 0 0 0]\n",
            "True:  [0 1 0 1 0 0 1 0]\n",
            "71  +  11  = str(out)\n",
            "------------\n",
            "Error:[2.53352328]\n",
            "Pred:  [1 0 1 0 0 0 1 0]\n",
            "True:  [1 1 0 0 0 0 1 0]\n",
            "81  +  113  = str(out)\n",
            "------------\n",
            "Error:[0.57691441]\n",
            "Pred:  [0 1 0 1 0 0 0 1]\n",
            "True:  [0 1 0 1 0 0 0 1]\n",
            "81  +  0  = str(out)\n",
            "------------\n",
            "Error:[1.42589952]\n",
            "Pred:  [1 0 0 0 0 0 0 1]\n",
            "True:  [1 0 0 0 0 0 0 1]\n",
            "4  +  125  = str(out)\n",
            "------------\n",
            "Error:[0.47477457]\n",
            "Pred:  [0 0 1 1 1 0 0 0]\n",
            "True:  [0 0 1 1 1 0 0 0]\n",
            "39  +  17  = str(out)\n",
            "------------\n",
            "Error:[0.21595037]\n",
            "Pred:  [0 0 0 0 1 1 1 0]\n",
            "True:  [0 0 0 0 1 1 1 0]\n",
            "11  +  3  = str(out)\n",
            "------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OHKYjGZEw98l"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}